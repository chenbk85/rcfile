/*
 *  fs/eventpoll.c (Efficient event retrieval implementation)
 *  Copyright (C) 2001,...,2009	 Davide Libenzi
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  Davide Libenzi <davidel@xmailserver.org>
 *
 */

#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/sched.h>
#include <linux/fs.h>
#include <linux/file.h>
#include <linux/signal.h>
#include <linux/errno.h>
#include <linux/mm.h>
#include <linux/slab.h>
#include <linux/poll.h>
#include <linux/string.h>
#include <linux/list.h>
#include <linux/hash.h>
#include <linux/spinlock.h>
#include <linux/syscalls.h>
#include <linux/rbtree.h>
#include <linux/wait.h>
/*#include <linux/eventpoll.h>*/
#include <linux/mount.h>
#include <linux/bitops.h>
#include <linux/mutex.h>
#include <linux/anon_inodes.h>
#include <asm/uaccess.h>
#include <asm/system.h>
#include <asm/io.h>
#include <asm/mman.h>
#include <asm/atomic.h>

#include "kepoll.h"


/*
 * LOCKING:
 * There are three level of locking required by k_epoll :
 *
 * 1) k_epmutex (mutex)
 * 2) k_ep->mtx (mutex)
 * 3) k_ep->lock (spinlock)
 *
 * The acquire order is the one listed above, from 1 to 3.
 * We need a spinlock (k_ep->lock) because we manipulate objects
 * from inside the poll callback, that might be triggered from
 * a wake_up() that in turn might be called from IRQ context.
 * So we can't slek_ep inside the poll callback and hence we need
 * a spinlock. During the event transfer loop (from kernel to
 * user space) we could end up slek_eping due a copy_to_user(), so
 * we need a lock that will allow us to slek_ep. This lock is a
 * mutex (k_ep->mtx). It is acquired during the event transfer loop,
 * during k_epoll_ctl(KEPOLL_CTL_DEL) and during eventpoll_release_file().
 * Then we also need a global mutex to serialize eventpoll_release_file()
 * and k_ep_free().
 * This mutex is acquired by k_ep_free() during the k_epoll file
 * cleanup path and it is also acquired by eventpoll_release_file()
 * if a file has been pushed inside an k_epoll set and it is then
 * close()d without a previous call tok_epoll_ctl(KEPOLL_CTL_DEL).
 * It is possible to drop the "k_ep->mtx" and to use the global
 * mutex "k_epmutex" (together with "k_ep->lock") to have it working,
 * but having "k_ep->mtx" will make the interface more scalable.
 * Events that require holding "k_epmutex" are very rare, while for
 * normal operations the k_epoll private "k_ep->mtx" will guarantee
 * a better scalability.
 */

/* Epoll private bits inside the event mask */
#define KEP_PRIVATE_BITS (KEPOLLONESHOT | KEPOLLET)

/* Maximum number of nesting allowed inside k_epoll sets */
#define KEP_MAX_NESTS 4

/* Maximum msec timeout value storeable in a long int */
#define KEP_MAX_MSTIMEO min(1000ULL * MAX_SCHEDULE_TIMEOUT / HZ, (LONG_MAX - 999ULL) / HZ)

#define KEP_MAX_EVENTS (INT_MAX / sizeof(struct k_epoll_event))

#define KEP_UNACTIVE_PTR ((void *) -1L)

#define KEP_ITEM_COST (sizeof(struct k_epitem) + sizeof(struct k_eppoll_entry))

struct k_epoll_filefd {
	struct file *file;
	int fd;
};

/*
 * Structure used to track possible nested calls, for too dek_ep recursions
 * and loop cycles.
 */
struct nested_call_node {
	struct list_head llink;
	void *cookie;
	void *ctx;
};

/*
 * This structure is used as collector for nested calls, to check for
 * maximum recursion dk_ept and loop cycles.
 */
struct nested_calls {
	struct list_head tasks_call_list;
	spinlock_t lock;
};

/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 */
struct k_epitem {
	/* RB tree node used to link this structure to the eventpoll RB tree */
	struct rb_node rbn;

	/* List header used to link this structure to the eventpoll ready list */
	struct list_head rdllink;

	/*
	 * Works together "struct eventpoll"->ovflist in kek_eping the
	 * single linked chain of items.
	 */
	struct k_epitem *next;

	/* The file descriptor information this item refers to */
	struct k_epoll_filefd ffd;

	/* Number of active wait queue attached to poll operations */
	int nwait;

	/* List containing poll wait queues */
	struct list_head pwqlist;

	/* The "container" of this item */
	struct eventpoll *k_ep;

	/* List header used to link this item to the "struct file" items list */
	struct list_head fllink;

	/* The structure that describe the interested events and the source fd */
	struct k_epoll_event event;
};

/*
 * This structure is stored inside the "private_data" member of the file
 * structure and rapresent the main data sructure for the eventpoll
 * interface.
 */
struct eventpoll {
	/* Protect the this structure access */
	spinlock_t lock;

	/*
	 * This mutex is used to ensure that files are not removed
	 * while k_epoll is using them. This is held during the event
	 * collection loop, the file cleanup path, the k_epoll file exit
	 * code and the ctl operations.
	 */
	struct mutex mtx;

	/* Wait queue used by sys_k_epoll_wait() */
	wait_queue_head_t wq;

	/* Wait queue used by file->poll() */
	wait_queue_head_t poll_wait;

	/* List of ready file descriptors */
	struct list_head rdllist;

	/* RB tree root used to store monitored fd structs */
	struct rb_root rbr;

	/*
	 * This is a single linked list that chains all the "struct k_epitem" that
	 * happened while transfering ready events to userspace w/out
	 * holding ->lock.
	 */
	struct k_epitem *ovflist;

	/* The user that created the eventpoll descriptor */
	struct user_struct *user;
};

/* Wait structure used by the poll hooks */
struct k_eppoll_entry {
	/* List header used to link this structure to the "struct k_epitem" */
	struct list_head llink;

	/* The "base" pointer is set to the container "struct k_epitem" */
	struct k_epitem *base;

	/*
	 * Wait queue item that will be linked to the target file wait
	 * queue head.
	 */
	wait_queue_t wait;

	/* The wait queue head that linked the "wait" wait queue item */
	wait_queue_head_t *whead;
};

/* Wrapper struct used by poll queueing */
struct k_ep_pqueue {
	poll_table pt;
	struct k_epitem *k_epi;
};

/* Used by the k_ep_send_events() function as callback private data */
struct k_ep_send_events_data {
	int maxevents;
	struct k_epoll_event __user *events;
};

/*
 * Configuration options available inside /proc/sys/fs/k_epoll/
 */
/* Maximum number of k_epoll watched descriptors, per user */
static int max_user_watches __read_mostly;

/*
 * This mutex is used to serialize k_ep_free() and eventpoll_release_file().
 */
static DEFINE_MUTEX(k_epmutex);

/* Used for safe wake up implementation */
static struct nested_calls poll_safewake_ncalls;

/* Used to call file's f_op->poll() under the nested calls boundaries */
static struct nested_calls poll_readywalk_ncalls;

/* Slab cache used to allocate "struct k_epitem" */
static struct kmem_cache *k_epi_cache __read_mostly;

/* Slab cache used to allocate "struct k_eppoll_entry" */
static struct kmem_cache *pwq_cache __read_mostly;

#ifdef CONFIG_SYSCTL

#include <linux/sysctl.h>

static int zero;

ctl_table k_epoll_table[] = {
	{
		.procname	= "max_user_watches",
		.data		= &max_user_watches,
		.maxlen		= sizeof(int),
		.mode		= 0644,
		.proc_handler	= proc_dointvec_minmax,
		.extra1		= &zero,
	},
	{ }
};
#endif /* CONFIG_SYSCTL */


/* Setup the structure that is used as key for the RB tree */
static inline void k_ep_set_ffd(struct k_epoll_filefd *ffd,
			      struct file *file, int fd)
{
	ffd->file = file;
	ffd->fd = fd;
}

/* Compare RB tree keys */
static inline int k_ep_cmp_ffd(struct k_epoll_filefd *p1,
			     struct k_epoll_filefd *p2)
{
	return (p1->file > p2->file ? +1:
	        (p1->file < p2->file ? -1 : p1->fd - p2->fd));
}

/* Tells us if the item is currently linked */
static inline int k_ep_is_linked(struct list_head *p)
{
	return !list_empty(p);
}

/* Get the "struct k_epitem" from a wait queue pointer */
static inline struct k_epitem *k_ep_item_from_wait(wait_queue_t *p)
{
	return container_of(p, struct k_eppoll_entry, wait)->base;
}

/* Get the "struct k_epitem" from an k_epoll queue wrapper */
static inline struct k_epitem *k_ep_item_from_k_epqueue(poll_table *p)
{
	return container_of(p, struct k_ep_pqueue, pt)->k_epi;
}

/* Tells if the k_epoll_ctl(2) operation needs an event copy from userspace */
static inline int k_ep_op_has_event(int op)
{
	return op != KEPOLL_CTL_DEL;
}

/* Initialize the poll safe wake up structure */
static void k_ep_nested_calls_init(struct nested_calls *ncalls)
{
	INIT_LIST_HEAD(&ncalls->tasks_call_list);
	spin_lock_init(&ncalls->lock);
}

/**
 * k_ep_call_nested - Perform a bound (possibly) nested call, by checking
 *                  that the recursion limit is not exceeded, and that
 *                  the same nested call (by the meaning of same cookie) is
 *                  no re-entered.
 *
 * @ncalls: Pointer to the nested_calls structure to be used for this call.
 * @max_nests: Maximum number of allowed nesting calls.
 * @nproc: Nested call core function pointer.
 * @priv: Opaque data to be passed to the @nproc callback.
 * @cookie: Cookie to be used to identify this nested call.
 * @ctx: This instance context.
 *
 * Returns: Returns the code returned by the @nproc callback, or -1 if
 *          the maximum recursion limit has been exceeded.
 */
static int k_ep_call_nested(struct nested_calls *ncalls, int max_nests,
			  int (*nproc)(void *, void *, int), void *priv,
			  void *cookie, void *ctx)
{
	int error, call_nests = 0;
	unsigned long flags;
	struct list_head *lsthead = &ncalls->tasks_call_list;
	struct nested_call_node *tncur;
	struct nested_call_node tnode;

	spin_lock_irqsave(&ncalls->lock, flags);

	/*
	 * Try to see if the current task is already inside this wakeup call.
	 * We use a list here, since the population inside this set is always
	 * very much limited.
	 */
	list_for_each_entry(tncur, lsthead, llink) {
		if (tncur->ctx == ctx &&
		    (tncur->cookie == cookie || ++call_nests > max_nests)) {
			/*
			 * Ops ... loop detected or maximum nest level reached.
			 * We abort this wake by breaking the cycle itself.
			 */
			error = -1;
			goto out_unlock;
		}
	}

	/* Add the current task and cookie to the list */
	tnode.ctx = ctx;
	tnode.cookie = cookie;
	list_add(&tnode.llink, lsthead);

	spin_unlock_irqrestore(&ncalls->lock, flags);

	/* Call the nested function */
	error = (*nproc)(priv, cookie, call_nests);

	/* Remove the current task from the list */
	spin_lock_irqsave(&ncalls->lock, flags);
	list_del(&tnode.llink);
out_unlock:
	spin_unlock_irqrestore(&ncalls->lock, flags);

	return error;
}

#ifdef CONFIG_DEBUG_LOCK_ALLOC
static inline void k_ep_wake_up_nested(wait_queue_head_t *wqueue,
				     unsigned long events, int subclass)
{
	unsigned long flags;

	spin_lock_irqsave_nested(&wqueue->lock, flags, subclass);
	wake_up_locked_poll(wqueue, events);
	spin_unlock_irqrestore(&wqueue->lock, flags);
}
#else
static inline void k_ep_wake_up_nested(wait_queue_head_t *wqueue,
				     unsigned long events, int subclass)
{
	wake_up_poll(wqueue, events);
}
#endif

static int k_ep_poll_wakeup_proc(void *priv, void *cookie, int call_nests)
{
	k_ep_wake_up_nested((wait_queue_head_t *) cookie, POLLIN,
			  1 + call_nests);
	return 0;
}

/*
 * Perform a safe wake up of the poll wait list. The problem is that
 * with the new callback'd wake up system, it is possible that the
 * poll callback is reentered from inside the call to wake_up() done
 * on the poll wait queue head. The rule is that we cannot reenter the
 * wake up code from the same task more than KEP_MAX_NESTS times,
 * and we cannot reenter the same wait queue head at all. This will
 * enable to have a hierarchy of k_epoll file descriptor of no more than
 * KEP_MAX_NESTS dek_ep.
 */
static void k_ep_poll_safewake(wait_queue_head_t *wq)
{
	int this_cpu = get_cpu();

	k_ep_call_nested(&poll_safewake_ncalls, KEP_MAX_NESTS,
		       k_ep_poll_wakeup_proc, NULL, wq, (void *) (long) this_cpu);

	put_cpu();
}

/*
 * This function unregisters poll callbacks from the associated file
 * descriptor.  Must be called with "mtx" held (or "k_epmutex" if called from
 * k_ep_free).
 */
static void k_ep_unregister_pollwait(struct eventpoll *k_ep, struct k_epitem *k_epi)
{
	struct list_head *lsthead = &k_epi->pwqlist;
	struct k_eppoll_entry *pwq;

	while (!list_empty(lsthead)) {
		pwq = list_first_entry(lsthead, struct k_eppoll_entry, llink);

		list_del(&pwq->llink);
		remove_wait_queue(pwq->whead, &pwq->wait);
		kmem_cache_free(pwq_cache, pwq);
	}
}

/**
 * k_ep_scan_ready_list - Scans the ready list in a way that makes possible for
 *                      the scan code, to call f_op->poll(). Also allows for
 *                      O(NumReady) performance.
 *
 * @k_ep: Pointer to the k_epoll private data structure.
 * @sproc: Pointer to the scan callback.
 * @priv: Private opaque data passed to the @sproc callback.
 *
 * Returns: The same integer error code returned by the @sproc callback.
 */
static int k_ep_scan_ready_list(struct eventpoll *k_ep,
			      int (*sproc)(struct eventpoll *,
					   struct list_head *, void *),
			      void *priv)
{
	int error, pwake = 0;
	unsigned long flags;
	struct k_epitem *k_epi, *nk_epi;
	LIST_HEAD(txlist);

	/*
	 * We need to lock this because we could be hit by
	 * eventpoll_release_file() and k_epoll_ctl().
	 */
	mutex_lock(&k_ep->mtx);

	/*
	 * Steal the ready list, and re-init the original one to the
	 * empty list. Also, set k_ep->ovflist to NULL so that events
	 * happening while looping w/out locks, are not lost. We cannot
	 * have the poll callback to queue directly on k_ep->rdllist,
	 * because we want the "sproc" callback to be able to do it
	 * in a lockless way.
	 */
	spin_lock_irqsave(&k_ep->lock, flags);
	list_splice_init(&k_ep->rdllist, &txlist);
	k_ep->ovflist = NULL;
	spin_unlock_irqrestore(&k_ep->lock, flags);

	/*
	 * Now call the callback function.
	 */
	error = (*sproc)(k_ep, &txlist, priv);

	spin_lock_irqsave(&k_ep->lock, flags);
	/*
	 * During the time we spent inside the "sproc" callback, some
	 * other events might have been queued by the poll callback.
	 * We re-insert them inside the main ready-list here.
	 */
	for (nk_epi = k_ep->ovflist; (k_epi = nk_epi) != NULL;
	     nk_epi = k_epi->next, k_epi->next = KEP_UNACTIVE_PTR) {
		/*
		 * We need to check if the item is already in the list.
		 * During the "sproc" callback execution time, items are
		 * queued into ->ovflist but the "txlist" might already
		 * contain them, and the list_splice() below takes care of them.
		 */
		if (!k_ep_is_linked(&k_epi->rdllink))
			list_add_tail(&k_epi->rdllink, &k_ep->rdllist);
	}
	/*
	 * We need to set back k_ep->ovflist to KEP_UNACTIVE_PTR, so that after
	 * releasing the lock, events will be queued in the normal way inside
	 * k_ep->rdllist.
	 */
	k_ep->ovflist = KEP_UNACTIVE_PTR;

	/*
	 * Quickly re-inject items left on "txlist".
	 */
	list_splice(&txlist, &k_ep->rdllist);

	if (!list_empty(&k_ep->rdllist)) {
		/*
		 * Wake up (if active) both the eventpoll wait list and
		 * the ->poll() wait list (delayed after we release the lock).
		 */
		if (waitqueue_active(&k_ep->wq))
			wake_up_locked(&k_ep->wq);
		if (waitqueue_active(&k_ep->poll_wait))
			pwake++;
	}
	spin_unlock_irqrestore(&k_ep->lock, flags);

	mutex_unlock(&k_ep->mtx);

	/* We have to call this outside the lock */
	if (pwake)
		k_ep_poll_safewake(&k_ep->poll_wait);

	return error;
}

/*
 * Removes a "struct k_epitem" from the eventpoll RB tree and deallocates
 * all the associated resources. Must be called with "mtx" held.
 */
static int k_ep_remove(struct eventpoll *k_ep, struct k_epitem *k_epi)
{
	unsigned long flags;
	struct file *file = k_epi->ffd.file;

	/*
	 * Removes poll wait queue hooks. We _have_ to do this without holding
	 * the "k_ep->lock" otherwise a deadlock might occur. This because of the
	 * sequence of the lock acquisition. Here we do "k_ep->lock" then the wait
	 * queue head lock when unregistering the wait queue. The wakeup callback
	 * will run by holding the wait queue head lock and will call our callback
	 * that will try to get "k_ep->lock".
	 */
	k_ep_unregister_pollwait(k_ep, k_epi);

	/* Remove the current item from the list of k_epoll hooks */
	spin_lock(&file->f_lock);
	if (k_ep_is_linked(&k_epi->fllink))
		list_del_init(&k_epi->fllink);
	spin_unlock(&file->f_lock);

	rb_erase(&k_epi->rbn, &k_ep->rbr);

	spin_lock_irqsave(&k_ep->lock, flags);
	if (k_ep_is_linked(&k_epi->rdllink))
		list_del_init(&k_epi->rdllink);
	spin_unlock_irqrestore(&k_ep->lock, flags);

	/* At this point it is safe to free the eventpoll item */
	kmem_cache_free(k_epi_cache, k_epi);

	atomic_dec(&k_ep->user->epoll_watches);

	return 0;
}

static void k_ep_free(struct eventpoll *k_ep)
{
	struct rb_node *rbp;
	struct k_epitem *k_epi;

	/* We need to release all tasks waiting for these file */
	if (waitqueue_active(&k_ep->poll_wait))
		k_ep_poll_safewake(&k_ep->poll_wait);

	/*
	 * We need to lock this because we could be hit by
	 * eventpoll_release_file() while we're freeing the "struct eventpoll".
	 * We do not need to hold "k_ep->mtx" here because the k_epoll file
	 * is on the way to be removed and no one has references to it
	 * anymore. The only hit might come from eventpoll_release_file() but
	 * holding "k_epmutex" is sufficent here.
	 */
	mutex_lock(&k_epmutex);

	/*
	 * Walks through the whole tree by unregistering poll callbacks.
	 */
	for (rbp = rb_first(&k_ep->rbr); rbp; rbp = rb_next(rbp)) {
		k_epi = rb_entry(rbp, struct k_epitem, rbn);

		k_ep_unregister_pollwait(k_ep, k_epi);
	}

	/*
	 * Walks through the whole tree by freeing each "struct k_epitem". At this
	 * point we are sure no poll callbacks will be lingering around, and also by
	 * holding "k_epmutex" we can be sure that no file cleanup code will hit
	 * us during this operation. So we can avoid the lock on "k_ep->lock".
	 */
	while ((rbp = rb_first(&k_ep->rbr)) != NULL) {
		k_epi = rb_entry(rbp, struct k_epitem, rbn);
		k_ep_remove(k_ep, k_epi);
	}

	mutex_unlock(&k_epmutex);
	mutex_destroy(&k_ep->mtx);
	free_uid(k_ep->user);
	kfree(k_ep);
}

static int k_ep_eventpoll_release(struct inode *inode, struct file *file)
{
	struct eventpoll *k_ep = file->private_data;

	if (k_ep)
		k_ep_free(k_ep);

	return 0;
}

static int k_ep_read_events_proc(struct eventpoll *k_ep, struct list_head *head,
			       void *priv)
{
	struct k_epitem *k_epi, *tmp;

	list_for_each_entry_safe(k_epi, tmp, head, rdllink) {
		if (k_epi->ffd.file->f_op->poll(k_epi->ffd.file, NULL) &
		    k_epi->event.events)
			return POLLIN | POLLRDNORM;
		else {
			/*
			 * Item has been dropped into the ready list by the poll
			 * callback, but it's not actually ready, as far as
			 * caller requested events goes. We can remove it here.
			 */
			list_del_init(&k_epi->rdllink);
		}
	}

	return 0;
}

static int k_ep_poll_readyevents_proc(void *priv, void *cookie, int call_nests)
{
	return k_ep_scan_ready_list(priv, k_ep_read_events_proc, NULL);
}

static unsigned int k_ep_eventpoll_poll(struct file *file, poll_table *wait)
{
	int pollflags;
	struct eventpoll *k_ep = file->private_data;

	/* Insert inside our poll wait queue */
	poll_wait(file, &k_ep->poll_wait, wait);

	/*
	 * Proceed to find out if wanted events are really available inside
	 * the ready list. This need to be done under k_ep_call_nested()
	 * supervision, since the call to f_op->poll() done on listed files
	 * could re-enter here.
	 */
	pollflags = k_ep_call_nested(&poll_readywalk_ncalls, KEP_MAX_NESTS,
				   k_ep_poll_readyevents_proc, k_ep, k_ep, current);

	return pollflags != -1 ? pollflags : 0;
}

/* File callbacks that implement the eventpoll file behaviour */
static const struct file_operations eventpoll_fops = {
	.release	= k_ep_eventpoll_release,
	.poll		= k_ep_eventpoll_poll
};

/* Fast test to see if the file is an evenpoll file */
static inline int is_file_k_epoll(struct file *f)
{
	return f->f_op == &eventpoll_fops;
}

/*
 * This is called from eventpoll_release() to unlink files from the eventpoll
 * interface. We need to have this facility to cleanup correctly files that are
 * closed without being removed from the eventpoll interface.
 */
void eventpoll_release_file(struct file *file)
{
	struct list_head *lsthead = &file->f_ep_links;
	struct eventpoll *k_ep;
	struct k_epitem *k_epi;

	/*
	 * We don't want to get "file->f_lock" because it is not
	 * necessary. It is not necessary because we're in the "struct file"
	 * cleanup path, and this means that noone is using this file anymore.
	 * So, for example, k_epoll_ctl() cannot hit here since if we reach this
	 * point, the file counter already went to zero and fget() would fail.
	 * The only hit might come from k_ep_free() but by holding the mutex
	 * will correctly serialize the operation. We do need to acquire
	 * "k_ep->mtx" after "k_epmutex" because k_ep_remove() requires it when called
	 * from anywhere but k_ep_free().
	 *
	 * Besides, k_ep_remove() acquires the lock, so we can't hold it here.
	 */
	mutex_lock(&k_epmutex);

	while (!list_empty(lsthead)) {
		k_epi = list_first_entry(lsthead, struct k_epitem, fllink);

		k_ep = k_epi->k_ep;
		list_del_init(&k_epi->fllink);
		mutex_lock(&k_ep->mtx);
		k_ep_remove(k_ep, k_epi);
		mutex_unlock(&k_ep->mtx);
	}

	mutex_unlock(&k_epmutex);
}

static int k_ep_alloc(struct eventpoll **pk_ep)
{
	int error;
	struct user_struct *user;
	struct eventpoll *k_ep;

	user = get_current_user();
	error = -ENOMEM;
	k_ep = kzalloc(sizeof(*k_ep), GFP_KERNEL);
	if (unlikely(!k_ep))
		goto free_uid;

	spin_lock_init(&k_ep->lock);
	mutex_init(&k_ep->mtx);
	init_waitqueue_head(&k_ep->wq);
	init_waitqueue_head(&k_ep->poll_wait);
	INIT_LIST_HEAD(&k_ep->rdllist);
	k_ep->rbr = RB_ROOT;
	k_ep->ovflist = KEP_UNACTIVE_PTR;
	k_ep->user = user;

	*pk_ep = k_ep;

	return 0;

free_uid:
	free_uid(user);
	return error;
}

/*
 * Search the file inside the eventpoll tree. The RB tree operations
 * are protected by the "mtx" mutex, and k_ep_find() must be called with
 * "mtx" held.
 */
static struct k_epitem *k_ep_find(struct eventpoll *k_ep, struct file *file, int fd)
{
	int kcmp;
	struct rb_node *rbp;
	struct k_epitem *k_epi, *k_epir = NULL;
	struct k_epoll_filefd ffd;

	k_ep_set_ffd(&ffd, file, fd);
	for (rbp = k_ep->rbr.rb_node; rbp; ) {
		k_epi = rb_entry(rbp, struct k_epitem, rbn);
		kcmp = k_ep_cmp_ffd(&ffd, &k_epi->ffd);
		if (kcmp > 0)
			rbp = rbp->rb_right;
		else if (kcmp < 0)
			rbp = rbp->rb_left;
		else {
			k_epir = k_epi;
			break;
		}
	}

	return k_epir;
}

/*
 * This is the callback that is passed to the wait queue wakeup
 * machanism. It is called by the stored file descriptors when they
 * have events to rk_eport.
 */
static int k_ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
	int pwake = 0;
	unsigned long flags;
	struct k_epitem *k_epi = k_ep_item_from_wait(wait);
	struct eventpoll *k_ep = k_epi->k_ep;

	spin_lock_irqsave(&k_ep->lock, flags);

	/*
	 * If the event mask does not contain any poll(2) event, we consider the
	 * descriptor to be disabled. This condition is likely the effect of the
	 * KEPOLLONESHOT bit that disables the descriptor when an event is received,
	 * until the next KEPOLL_CTL_MOD will be issued.
	 */
	if (!(k_epi->event.events & ~KEP_PRIVATE_BITS))
		goto out_unlock;

	/*
	 * Check the events coming with the callback. At this stage, not
	 * every device rk_eports the events in the "key" parameter of the
	 * callback. We need to be able to handle both cases here, hence the
	 * test for "key" != NULL before the event match test.
	 */
	if (key && !((unsigned long) key & k_epi->event.events))
		goto out_unlock;

	/*
	 * If we are trasfering events to userspace, we can hold no locks
	 * (because we're accessing user memory, and because of linux f_op->poll()
	 * semantics). All the events that happens during that period of time are
	 * chained in k_ep->ovflist and requeued later on.
	 */
	if (unlikely(k_ep->ovflist != KEP_UNACTIVE_PTR)) {
		if (k_epi->next == KEP_UNACTIVE_PTR) {
			k_epi->next = k_ep->ovflist;
			k_ep->ovflist = k_epi;
		}
		goto out_unlock;
	}

	/* If this file is already in the ready list we exit soon */
	if (!k_ep_is_linked(&k_epi->rdllink))
		list_add_tail(&k_epi->rdllink, &k_ep->rdllist);

	/*
	 * Wake up ( if active ) both the eventpoll wait list and the ->poll()
	 * wait list.
	 */
	if (waitqueue_active(&k_ep->wq))
		wake_up_locked(&k_ep->wq);
	if (waitqueue_active(&k_ep->poll_wait))
		pwake++;

out_unlock:
	spin_unlock_irqrestore(&k_ep->lock, flags);

	/* We have to call this outside the lock */
	if (pwake)
		k_ep_poll_safewake(&k_ep->poll_wait);

	return 1;
}

/*
 * This is the callback that is used to add our wait queue to the
 * target file wakeup lists.
 */
static void k_ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
				 poll_table *pt)
{
	struct k_epitem *k_epi = k_ep_item_from_k_epqueue(pt);
	struct k_eppoll_entry *pwq;

	if (k_epi->nwait >= 0 && (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
		init_waitqueue_func_entry(&pwq->wait, k_ep_poll_callback);
		pwq->whead = whead;
		pwq->base = k_epi;
		add_wait_queue(whead, &pwq->wait);
		list_add_tail(&pwq->llink, &k_epi->pwqlist);
		k_epi->nwait++;
	} else {
		/* We have to signal that an error occurred */
		k_epi->nwait = -1;
	}
}

static void k_ep_rbtree_insert(struct eventpoll *k_ep, struct k_epitem *k_epi)
{
	int kcmp;
	struct rb_node **p = &k_ep->rbr.rb_node, *parent = NULL;
	struct k_epitem *k_epic;

	while (*p) {
		parent = *p;
		k_epic = rb_entry(parent, struct k_epitem, rbn);
		kcmp = k_ep_cmp_ffd(&k_epi->ffd, &k_epic->ffd);
		if (kcmp > 0)
			p = &parent->rb_right;
		else
			p = &parent->rb_left;
	}
	rb_link_node(&k_epi->rbn, parent, p);
	rb_insert_color(&k_epi->rbn, &k_ep->rbr);
}

/*
 * Must be called with "mtx" held.
 */
static int k_ep_insert(struct eventpoll *k_ep, struct k_epoll_event *event,
		     struct file *tfile, int fd)
{
	int error, revents, pwake = 0;
	unsigned long flags;
	struct k_epitem *k_epi;
	struct k_ep_pqueue k_epq;

	if (unlikely(atomic_read(&k_ep->user->epoll_watches) >=
		     max_user_watches))
		return -ENOSPC;
	if (!(k_epi = kmem_cache_alloc(k_epi_cache, GFP_KERNEL)))
		return -ENOMEM;

	/* Item initialization follow here ... */
	INIT_LIST_HEAD(&k_epi->rdllink);
	INIT_LIST_HEAD(&k_epi->fllink);
	INIT_LIST_HEAD(&k_epi->pwqlist);
	k_epi->k_ep = k_ep;
	k_ep_set_ffd(&k_epi->ffd, tfile, fd);
	k_epi->event = *event;
	k_epi->nwait = 0;
	k_epi->next = KEP_UNACTIVE_PTR;

	/* Initialize the poll table using the queue callback */
	k_epq.k_epi = k_epi;
	init_poll_funcptr(&k_epq.pt, k_ep_ptable_queue_proc);

	/*
	 * Attach the item to the poll hooks and get current event bits.
	 * We can safely use the file* here because its usage count has
	 * been increased by the caller of this function. Note that after
	 * this operation completes, the poll callback can start hitting
	 * the new item.
	 */
	revents = tfile->f_op->poll(tfile, &k_epq.pt);

	/*
	 * We have to check if something went wrong during the poll wait queue
	 * install process. Namely an allocation for a wait queue failed due
	 * high memory pressure.
	 */
	error = -ENOMEM;
	if (k_epi->nwait < 0)
		goto error_unregister;

	/* Add the current item to the list of active k_epoll hook for this file */
	spin_lock(&tfile->f_lock);
	list_add_tail(&k_epi->fllink, &tfile->f_ep_links);/*attention and release_file*/
	spin_unlock(&tfile->f_lock);

	/*
	 * Add the current item to the RB tree. All RB tree operations are
	 * protected by "mtx", and k_ep_insert() is called with "mtx" held.
	 */
	k_ep_rbtree_insert(k_ep, k_epi);

	/* We have to drop the new item inside our item list to kek_ep track of it */
	spin_lock_irqsave(&k_ep->lock, flags);

	/* If the file is already "ready" we drop it inside the ready list */
	if ((revents & event->events) && !k_ep_is_linked(&k_epi->rdllink)) {
		list_add_tail(&k_epi->rdllink, &k_ep->rdllist);

		/* Notify waiting tasks that events are available */
		if (waitqueue_active(&k_ep->wq))
			wake_up_locked(&k_ep->wq);
		if (waitqueue_active(&k_ep->poll_wait))
			pwake++;
	}

	spin_unlock_irqrestore(&k_ep->lock, flags);

	atomic_inc(&k_ep->user->epoll_watches);

	/* We have to call this outside the lock */
	if (pwake)
		k_ep_poll_safewake(&k_ep->poll_wait);

	return 0;

error_unregister:
	k_ep_unregister_pollwait(k_ep, k_epi);

	/*
	 * We need to do this because an event could have been arrived on some
	 * allocated wait queue. Note that we don't care about the k_ep->ovflist
	 * list, since that is used/cleaned only inside a section bound by "mtx".
	 * And k_ep_insert() is called with "mtx" held.
	 */
	spin_lock_irqsave(&k_ep->lock, flags);
	if (k_ep_is_linked(&k_epi->rdllink))
		list_del_init(&k_epi->rdllink);
	spin_unlock_irqrestore(&k_ep->lock, flags);

	kmem_cache_free(k_epi_cache, k_epi);

	return error;
}

/*
 * Modify the interest event mask by dropping an event if the new mask
 * has a match in the current file status. Must be called with "mtx" held.
 */
static int k_ep_modify(struct eventpoll *k_ep, struct k_epitem *k_epi, struct k_epoll_event *event)
{
	int pwake = 0;
	unsigned int revents;

	/*
	 * Set the new event interest mask before calling f_op->poll();
	 * otherwise we might miss an event that happens between the
	 * f_op->poll() call and the new event set registering.
	 */
	k_epi->event.events = event->events;
	k_epi->event.data = event->data; /* protected by mtx */

	/*
	 * Get current event bits. We can safely use the file* here because
	 * its usage count has been increased by the caller of this function.
	 */
	revents = k_epi->ffd.file->f_op->poll(k_epi->ffd.file, NULL);

	/*
	 * If the item is "hot" and it is not registered inside the ready
	 * list, push it inside.
	 */
	if (revents & event->events) {
		spin_lock_irq(&k_ep->lock);
		if (!k_ep_is_linked(&k_epi->rdllink)) {
			list_add_tail(&k_epi->rdllink, &k_ep->rdllist);

			/* Notify waiting tasks that events are available */
			if (waitqueue_active(&k_ep->wq))
				wake_up_locked(&k_ep->wq);
			if (waitqueue_active(&k_ep->poll_wait))
				pwake++;
		}
		spin_unlock_irq(&k_ep->lock);
	}

	/* We have to call this outside the lock */
	if (pwake)
		k_ep_poll_safewake(&k_ep->poll_wait);

	return 0;
}

static int k_ep_send_events_proc(struct eventpoll *k_ep, struct list_head *head,
			       void *priv)
{
	struct k_ep_send_events_data *esed = priv;
	int eventcnt;
	unsigned int revents;
	struct k_epitem *k_epi;
	struct k_epoll_event __user *uevent;

	/*
	 * We can loop without lock because we are passed a task private list.
	 * Items cannot vanish during the loop because k_ep_scan_ready_list() is
	 * holding "mtx" during this call.
	 */
	for (eventcnt = 0, uevent = esed->events;
	     !list_empty(head) && eventcnt < esed->maxevents;) {
		k_epi = list_first_entry(head, struct k_epitem, rdllink);

		list_del_init(&k_epi->rdllink);

		revents = k_epi->ffd.file->f_op->poll(k_epi->ffd.file, NULL) &
			k_epi->event.events;

		/*
		 * If the event mask intersect the caller-requested one,
		 * deliver the event to userspace. Again, k_ep_scan_ready_list()
		 * is holding "mtx", so no operations coming from userspace
		 * can change the item.
		 */
		if (revents) {
			if (__put_user(revents, &uevent->events) ||
			    __put_user(k_epi->event.data, &uevent->data)) {
				list_add(&k_epi->rdllink, head);
				return eventcnt ? eventcnt : -EFAULT;
			}
			eventcnt++;
			uevent++;
			if (k_epi->event.events & KEPOLLONESHOT)
				k_epi->event.events &= KEP_PRIVATE_BITS;
			else if (!(k_epi->event.events & KEPOLLET)) {
				/*
				 * If this file has been added with Level
				 * Trigger mode, we need to insert back inside
				 * the ready list, so that the next call to
				 * k_epoll_wait() will check again the events
				 * availability. At this point, noone can insert
				 * into k_ep->rdllist besides us. The k_epoll_ctl()
				 * callers are locked out by
				 * k_ep_scan_ready_list() holding "mtx" and the
				 * poll callback will queue them in k_ep->ovflist.
				 */
				list_add_tail(&k_epi->rdllink, &k_ep->rdllist);
			}
		}
	}

	return eventcnt;
}

static int k_ep_send_events(struct eventpoll *k_ep,
			  struct k_epoll_event __user *events, int maxevents)
{
	struct k_ep_send_events_data esed;

	esed.maxevents = maxevents;
	esed.events = events;

	return k_ep_scan_ready_list(k_ep, k_ep_send_events_proc, &esed);
}

static int k_ep_poll(struct eventpoll *k_ep, struct k_epoll_event __user *events,
		   int maxevents, long timeout)
{
	int res, eavail;
	unsigned long flags;
	long jtimeout;
	wait_queue_t wait;

	/*
	 * Calculate the timeout by checking for the "infinite" value (-1)
	 * and the overflow condition. The passed timeout is in milliseconds,
	 * that why (t * HZ) / 1000.
	 */
	jtimeout = (timeout < 0 || timeout >= KEP_MAX_MSTIMEO) ?
		MAX_SCHEDULE_TIMEOUT : (timeout * HZ + 999) / 1000;

retry:
	spin_lock_irqsave(&k_ep->lock, flags);

	res = 0;
	if (list_empty(&k_ep->rdllist)) {
		/*
		 * We don't have any available event to return to the caller.
		 * We need to slek_ep here, and we will be wake up by
		 * k_ep_poll_callback() when events will become available.
		 */
		init_waitqueue_entry(&wait, current);
		__add_wait_queue_exclusive(&k_ep->wq, &wait);

		for (;;) {
			/*
			 * We don't want to slek_ep if the k_ep_poll_callback() sends us
			 * a wakeup in between. That's why we set the task state
			 * to TASK_INTERRUPTIBLE before doing the checks.
			 */
			set_current_state(TASK_INTERRUPTIBLE);
			if (!list_empty(&k_ep->rdllist) || !jtimeout)
				break;
			if (signal_pending(current)) {
				res = -EINTR;
				break;
			}

			spin_unlock_irqrestore(&k_ep->lock, flags);
			jtimeout = schedule_timeout(jtimeout);
			spin_lock_irqsave(&k_ep->lock, flags);
		}
		__remove_wait_queue(&k_ep->wq, &wait);

		set_current_state(TASK_RUNNING);
	}
	/* Is it worth to try to dig for events ? */
	eavail = !list_empty(&k_ep->rdllist) || k_ep->ovflist != KEP_UNACTIVE_PTR;

	spin_unlock_irqrestore(&k_ep->lock, flags);

	/*
	 * Try to transfer events to user space. In case we get 0 events and
	 * there's still timeout left over, we go trying again in search of
	 * more luck.
	 */
	if (!res && eavail &&
	    !(res = k_ep_send_events(k_ep, events, maxevents)) && jtimeout)
		goto retry;

	return res;
}

/*
 * Open an eventpoll fd
 */

int k_epoll_create(int flags)
{
	int error;
	struct eventpoll *k_ep = NULL;

	/* Check the KEPOLL_* constant for consistency.  */
	BUILD_BUG_ON(KEPOLL_CLOEXEC != O_CLOEXEC);

	if (flags & ~KEPOLL_CLOEXEC)
		return -EINVAL;
	/*
	 * Create the internal data structure ("struct eventpoll").
	 */
	error = k_ep_alloc(&k_ep);
	if (error < 0)
		return error;
	/*
	 * Creates all the items needed to setup an eventpoll file. That is,
	 * a file structure and a free file descriptor.
	 */
	error = anon_inode_getfd("[eventpoll]", &eventpoll_fops, k_ep,
				 O_RDWR | (flags & O_CLOEXEC));
	if (error < 0)
		k_ep_free(k_ep);

	return error;
}

int k_epoll_ctl(int k_epfd, int op, int fd,
		struct k_epoll_event * event)

{
	int error;
	struct file *file, *tfile;
	struct eventpoll *k_ep;
	struct k_epitem *k_epi;
	struct k_epoll_event k_epds;

	error = -EFAULT;
	if (k_ep_op_has_event(op) &&
	    copy_from_user(&k_epds, event, sizeof(struct k_epoll_event)))
		goto error_return;

	/* Get the "struct file *" for the eventpoll file */
	error = -EBADF;
	file = fget(k_epfd);
	if (!file)
		goto error_return;

	/* Get the "struct file *" for the target file */
	tfile = fget(fd);
	if (!tfile)
		goto error_fput;

	/* The target file descriptor must support poll */
	error = -EPERM;
	if (!tfile->f_op || !tfile->f_op->poll)
		goto error_tgt_fput;

	/*
	 * We have to check that the file structure underneath the file descriptor
	 * the user passed to us _is_ an eventpoll file. And also we do not permit
	 * adding an k_epoll file descriptor inside itself.
	 */
	error = -EINVAL;
	if (file == tfile || !is_file_k_epoll(file))
		goto error_tgt_fput;

	/*
	 * At this point it is safe to assume that the "private_data" contains
	 * our own data structure.
	 */
	k_ep = file->private_data;

	mutex_lock(&k_ep->mtx);

	/*
	 * Try to lookup the file inside our RB tree, Since we grabbed "mtx"
	 * above, we can be sure to be able to use the item looked up by
	 * k_ep_find() till we release the mutex.
	 */
	k_epi = k_ep_find(k_ep, tfile, fd);

	error = -EINVAL;
	switch (op) {
	case KEPOLL_CTL_ADD:
		if (!k_epi) {
			k_epds.events |= POLLERR | POLLHUP;
			error = k_ep_insert(k_ep, &k_epds, tfile, fd);
		} else
			error = -EEXIST;
		break;
	case KEPOLL_CTL_DEL:
		if (k_epi)
			error = k_ep_remove(k_ep, k_epi);
		else
			error = -ENOENT;
		break;
	case KEPOLL_CTL_MOD:
		if (k_epi) {
			k_epds.events |= POLLERR | POLLHUP;
			error = k_ep_modify(k_ep, k_epi, &k_epds);
		} else
			error = -ENOENT;
		break;
	}
	mutex_unlock(&k_ep->mtx);

error_tgt_fput:
	fput(tfile);
error_fput:
	fput(file);
error_return:

	return error;
	return -1;
}

int k_epoll_wait(int k_epfd, struct k_epoll_event *events,
		int maxevents, int timeout)
{
	int error;
	struct file *file;
	struct eventpoll *k_ep;

	/* The maximum number of event must be greater than zero */
	if (maxevents <= 0 || maxevents > KEP_MAX_EVENTS)
		return -EINVAL;

	/* Verify that the area passed by the user is writeable */
	if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct k_epoll_event))) {
		error = -EFAULT;
		goto error_return;
	}

	/* Get the "struct file *" for the eventpoll file */
	error = -EBADF;
	file = fget(k_epfd);
	if (!file)
		goto error_return;

	/*
	 * We have to check that the file structure underneath the fd
	 * the user passed to us _is_ an eventpoll file.
	 */
	error = -EINVAL;
	if (!is_file_k_epoll(file))
		goto error_fput;

	/*
	 * At this point it is safe to assume that the "private_data" contains
	 * our own data structure.
	 */
	k_ep = file->private_data;

	/* Time to fish for events ... */
	error = k_ep_poll(k_ep, events, maxevents, timeout);

error_fput:
	fput(file);
error_return:

	return error;
}


/*
 * Open an eventpoll file descriptor.
 */
SYSCALL_DEFINE1(k_epoll_create1, int, flags)
{
	int error;
	struct eventpoll *k_ep = NULL;

	/* Check the KEPOLL_* constant for consistency.  */
	BUILD_BUG_ON(KEPOLL_CLOEXEC != O_CLOEXEC);

	if (flags & ~KEPOLL_CLOEXEC)
		return -EINVAL;
	/*
	 * Create the internal data structure ("struct eventpoll").
	 */
	error = k_ep_alloc(&k_ep);
	if (error < 0)
		return error;
	/*
	 * Creates all the items needed to setup an eventpoll file. That is,
	 * a file structure and a free file descriptor.
	 */
	error = anon_inode_getfd("[eventpoll]", &eventpoll_fops, k_ep,
				 O_RDWR | (flags & O_CLOEXEC));
	if (error < 0)
		k_ep_free(k_ep);

	return error;
}

SYSCALL_DEFINE1(k_epoll_create, int, size)
{
	if (size <= 0)
		return -EINVAL;

	return sys_k_epoll_create1(0);
}

/*
 * The following function implements the controller interface for
 * the eventpoll file that enables the insertion/removal/change of
 * file descriptors inside the interest set.
 */
SYSCALL_DEFINE4(k_epoll_ctl, int, k_epfd, int, op, int, fd,
		struct k_epoll_event __user *, event)
{
	int error;
	struct file *file, *tfile;
	struct eventpoll *k_ep;
	struct k_epitem *k_epi;
	struct k_epoll_event k_epds;

	error = -EFAULT;
	if (k_ep_op_has_event(op) &&
	    copy_from_user(&k_epds, event, sizeof(struct k_epoll_event)))
		goto error_return;

	/* Get the "struct file *" for the eventpoll file */
	error = -EBADF;
	file = fget(k_epfd);
	if (!file)
		goto error_return;

	/* Get the "struct file *" for the target file */
	tfile = fget(fd);
	if (!tfile)
		goto error_fput;

	/* The target file descriptor must support poll */
	error = -EPERM;
	if (!tfile->f_op || !tfile->f_op->poll)
		goto error_tgt_fput;

	/*
	 * We have to check that the file structure underneath the file descriptor
	 * the user passed to us _is_ an eventpoll file. And also we do not permit
	 * adding an k_epoll file descriptor inside itself.
	 */
	error = -EINVAL;
	if (file == tfile || !is_file_k_epoll(file))
		goto error_tgt_fput;

	/*
	 * At this point it is safe to assume that the "private_data" contains
	 * our own data structure.
	 */
	k_ep = file->private_data;

	mutex_lock(&k_ep->mtx);

	/*
	 * Try to lookup the file inside our RB tree, Since we grabbed "mtx"
	 * above, we can be sure to be able to use the item looked up by
	 * k_ep_find() till we release the mutex.
	 */
	k_epi = k_ep_find(k_ep, tfile, fd);

	error = -EINVAL;
	switch (op) {
	case KEPOLL_CTL_ADD:
		if (!k_epi) {
			k_epds.events |= POLLERR | POLLHUP;
			error = k_ep_insert(k_ep, &k_epds, tfile, fd);
		} else
			error = -EEXIST;
		break;
	case KEPOLL_CTL_DEL:
		if (k_epi)
			error = k_ep_remove(k_ep, k_epi);
		else
			error = -ENOENT;
		break;
	case KEPOLL_CTL_MOD:
		if (k_epi) {
			k_epds.events |= POLLERR | POLLHUP;
			error = k_ep_modify(k_ep, k_epi, &k_epds);
		} else
			error = -ENOENT;
		break;
	}
	mutex_unlock(&k_ep->mtx);

error_tgt_fput:
	fput(tfile);
error_fput:
	fput(file);
error_return:

	return error;
}

/*
 * Implement the event wait interface for the eventpoll file. It is the kernel
 * part of the user space k_epoll_wait(2).
 */
SYSCALL_DEFINE4(k_epoll_wait, int, k_epfd, struct k_epoll_event __user *, events,
		int, maxevents, int, timeout)
{
	int error;
	struct file *file;
	struct eventpoll *k_ep;

	/* The maximum number of event must be greater than zero */
	if (maxevents <= 0 || maxevents > KEP_MAX_EVENTS)
		return -EINVAL;

	/* Verify that the area passed by the user is writeable */
	if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct k_epoll_event))) {
		error = -EFAULT;
		goto error_return;
	}

	/* Get the "struct file *" for the eventpoll file */
	error = -EBADF;
	file = fget(k_epfd);
	if (!file)
		goto error_return;

	/*
	 * We have to check that the file structure underneath the fd
	 * the user passed to us _is_ an eventpoll file.
	 */
	error = -EINVAL;
	if (!is_file_k_epoll(file))
		goto error_fput;

	/*
	 * At this point it is safe to assume that the "private_data" contains
	 * our own data structure.
	 */
	k_ep = file->private_data;

	/* Time to fish for events ... */
	error = k_ep_poll(k_ep, events, maxevents, timeout);

error_fput:
	fput(file);
error_return:

	return error;
}

#ifdef HAVE_SET_RESTORE_SIGMASK

/*
 * Implement the event wait interface for the eventpoll file. It is the kernel
 * part of the user space k_epoll_pwait(2).
 */
SYSCALL_DEFINE6(k_epoll_pwait, int, k_epfd, struct k_epoll_event __user *, events,
		int, maxevents, int, timeout, const sigset_t __user *, sigmask,
		size_t, sigsetsize)
{
	int error;
	sigset_t ksigmask, sigsaved;

	/*
	 * If the caller wants a certain signal mask to be set during the wait,
	 * we apply it here.
	 */
	if (sigmask) {
		if (sigsetsize != sizeof(sigset_t))
			return -EINVAL;
		if (copy_from_user(&ksigmask, sigmask, sizeof(ksigmask)))
			return -EFAULT;
		sigdelsetmask(&ksigmask, sigmask(SIGKILL) | sigmask(SIGSTOP));
		sigprocmask(SIG_SETMASK, &ksigmask, &sigsaved);
	}

	error = sys_k_epoll_wait(k_epfd, events, maxevents, timeout);

	/*
	 * If we changed the signal mask, we need to restore the original one.
	 * In case we've got a signal while waiting, we do not restore the
	 * signal mask yet, and we allow do_signal() to deliver the signal on
	 * the way back to userspace, before the signal mask is restored.
	 */
	if (sigmask) {
		if (error == -EINTR) {
			memcpy(&current->saved_sigmask, &sigsaved,
			       sizeof(sigsaved));
			set_restore_sigmask();
		} else
			sigprocmask(SIG_SETMASK, &sigsaved, NULL);
	}

	return error;
}

#endif /* HAVE_SET_RESTORE_SIGMASK */

static int __init eventpoll_init(void)
{
	struct sysinfo si;

	si_meminfo(&si);
	/*
	 * Allows top 4% of lomem to be allocated for k_epoll watches (per user).
	 */
	max_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /
		KEP_ITEM_COST;

	/* Initialize the structure used to perform safe poll wait head wake ups */
	k_ep_nested_calls_init(&poll_safewake_ncalls);

	/* Initialize the structure used to perform file's f_op->poll() calls */
	k_ep_nested_calls_init(&poll_readywalk_ncalls);

	/* Allocates slab cache used to allocate "struct k_epitem" items */
	k_epi_cache = kmem_cache_create("eventpoll_k_epi", sizeof(struct k_epitem),
			0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);

	/* Allocates slab cache used to allocate "struct k_eppoll_entry" */
	pwq_cache = kmem_cache_create("eventpoll_pwq",
			sizeof(struct k_eppoll_entry), 0, SLAB_PANIC, NULL);

	return 0;
}
fs_initcall(eventpoll_init);


